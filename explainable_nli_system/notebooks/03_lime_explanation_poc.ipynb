{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof-of-Concept: LIME Explanations for NLI Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src') # Add src directory to path to find xai_utils\n",
    "\n",
    "import lime\n",
    "import lime.lime_text\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "import numpy as np\n",
    "import os\n",
    "from xai_utils import get_lime_predictor # Import the utility function\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Fine-tuned Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '../src/nli_model/' # Adjusted path relative to notebook location\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_dir)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_dir)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Sample Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premise = \"A man is playing a guitar.\"\n",
    "hypothesis = \"A person is making music.\"\n",
    "text_instance = premise + \" \" + tokenizer.sep_token + \" \" + hypothesis\n",
    "\n",
    "print(f\"Input text instance for LIME: '{text_instance}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize LIME TextExplainer and Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['entailment', 'contradiction', 'neutral']\n",
    "explainer = lime.lime_text.LimeTextExplainer(class_names=class_names, bow=False, random_state=42)\n",
    "\n",
    "# Get the predictor function from our utility module\n",
    "predictor_fn = get_lime_predictor(model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate LIME Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's see the model's prediction for the instance using our predictor_fn\n",
    "pred_probs = predictor_fn([text_instance]) # Use the new predictor_fn\n",
    "predicted_label_idx = np.argmax(pred_probs[0])\n",
    "predicted_label_str = class_names[predicted_label_idx]\n",
    "print(f\"Model's direct prediction for LIME input:\")\n",
    "print(f\"  Text: '{text_instance}'\")\n",
    "print(f\"  Predicted Label: {predicted_label_str} (Index: {predicted_label_idx})\")\n",
    "print(f\"  Probabilities: {pred_probs[0]}\")\n",
    "\n",
    "# Generate the explanation\n",
    "explanation = explainer.explain_instance(\n",
    "    text_instance,\n",
    "    predictor_fn, # Use the new predictor_fn\n",
    "    num_features=10,\n",
    "    num_samples=500\n",
    ")\n",
    "print(\"\\nLIME explanation generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Present Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Displaying LIME explanation in notebook:\")\n",
    "explanation.show_in_notebook(text=True)\n",
    "\n",
    "print(\"\\nLIME explanation as a list of (word, weight) tuples:\")\n",
    "print(explanation.as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interpretation Note for LIME Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIME (Local Interpretable Model-agnostic Explanations) explains the prediction of a specific instance by approximating the complex model locally with a simpler, interpretable model (e.g., a weighted linear model).\n",
    "\n",
    "**How to Interpret the Output Above (`show_in_notebook`):**\n",
    "\n",
    "- **Predicted Label and Probabilities:** At the top, LIME shows the label it is explaining (which should match our model's prediction) and the probability scores for each class for that specific instance.\n",
    "- **Highlighted Text:** The input text is displayed with words highlighted. \n",
    "  - **Color Coding:** Words are colored based on their contribution to the predicted label.\n",
    "    - Typically, **green (or positive color)** words contribute *towards* the predicted label.\n",
    "    - **Red (or negative color)** words contribute *against* the predicted label (i.e., they support other labels).\n",
    "    - The intensity of the color often indicates the strength of the contribution.\n",
    "  - **Word Importance:** The highlighted words are those that LIME's local linear model found to be most influential for the prediction of the instance. The `num_features` parameter in `explain_instance` controls how many such words are shown.\n",
    "\n",
    "**How to Interpret the Output (`as_list()`):**\n",
    "\n",
    "- This provides a list of tuples, where each tuple is `(word, weight)`.\n",
    "- `word`: The feature (word) identified by LIME.\n",
    "- `weight`: A numerical score representing the importance and direction of the word's contribution to the predicted label.\n",
    "  - **Positive weights** indicate that the presence of the word increases the probability of the predicted label.\n",
    "  - **Negative weights** indicate that the presence of the word decreases the probability of the predicted label (or increases the probability of other labels).\n",
    "  - The magnitude of the weight indicates the strength of the contribution.\n",
    "\n",
    "**Key Considerations:**\n",
    "- **Locality:** LIME explanations are local. They explain why the model made a specific prediction for a *particular instance*, not how the model behaves globally.\n",
    "- **Faithfulness vs. Interpretability:** LIME makes a trade-off. The local model is simpler and interpretable but is only an approximation of the complex model's behavior in that local region.\n",
    "- **Perturbation Strategy:** LIME works by creating perturbed samples (e.g., by removing words from the input instance) and observing how the model's predictions change. The `bow=False` setting is important for transformer models, as LIME's default assumption of a bag-of-words model is not accurate for them. Even with `bow=False`, the perturbation still involves removing tokens/words, which is a simplification of how transformers process text structure.\n",
    "- **`[SEP]` Token:** The `[SEP]` token might be treated as a regular word by LIME's default splitter. Its importance score can sometimes reflect its role in separating premise and hypothesis for the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
